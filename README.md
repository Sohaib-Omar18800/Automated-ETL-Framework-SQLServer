# <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> Soda-Powered Medallion Data Pipeline

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![SQL Server](https://img.shields.io/badge/Microsoft%20SQL%20Server-CC2927?style=for-the-badge&logo=microsoft-sql-server&logoColor=white)
![Polars](https://img.shields.io/badge/Polars-%23CD792C?style=for-the-badge&logo=polars&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-FFF000?style=for-the-badge&logo=duckdb&logoColor=black)
![Soda](https://img.shields.io/badge/Soda.io-00D1FF?style=for-the-badge&logo=soda&logoColor=white)

An advanced, robust ETL pipeline implementing the **Medallion Architecture** (Bronze, Silver, Gold). This project focuses on **Incremental Loading**, **Data Quality Assurance**, and **Self-Healing Infrastructure**.

---

## ğŸ—ï¸ Architecture Workflow

Here is the high-level data flow from source to consumption:

<div align="center">
  <img src="./Docs/Data%20Architecture.png" alt="Data Architecture Workflow" width="900">
  <p><i>Figure 1: Medallion Architecture with Polars & Soda Integration</i></p>
</div>

---

## ğŸ“‚ Project Structure

```text
Automated-ETL-Framework-SQLServer/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ source_crm
â”‚   â””â”€â”€ source_erp
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Data_Architecture.png # Architecture Diagram
â”‚   â”œâ”€â”€ DWH_Schema.png        # Schema of DWH
â”‚   â”œâ”€â”€ Layers,Tables&dim.png # Connection Between Layers
â”‚   â”œâ”€â”€ Relations_Between_Tables.png # Relations Diagram
â”‚   â””â”€â”€ data_catalog.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze
â”‚     â””â”€â”€ ddl_bronze_tables.sql
â”‚   â”œâ”€â”€ silver
â”‚     â””â”€â”€ ddl_silver_tables.sql
â”‚   â”œâ”€â”€ gold
â”‚     â””â”€â”€ ddl_gold_tables.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/
â”‚     â”œâ”€â”€ start_bronze.py      # Database initialization & Bronze logic
â”‚     â”œâ”€â”€ start_silver.py      # Schema creation & Silver dependency checks
â”‚     â””â”€â”€ start_gold.py        # Transformation & Star Schema in DuckDB
â”‚   â”œâ”€â”€ engine/
â”‚     â””â”€â”€ auto_increment.py    # The core ETL engine (Incremental Load logic)
â”‚   â”œâ”€â”€ config/
â”‚     â”œâ”€â”€ create_bronze_table.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ create_silver_table.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ create_gold_table.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ etl_bronze_to_silver.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ schemas.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ soda_config.py           # Soda YAML files for integrity checks
â”‚     â”œâ”€â”€ sql_service_var.py           # Soda YAML files for integrity checks
â”‚     â””â”€â”€ tables.py           # Soda YAML files for integrity checks
â”‚   â”œâ”€â”€ test/
â”‚     â”œâ”€â”€ soda_check/
â”‚     â”œâ”€â”€ crm_customer_info_test.py
â”‚     â”œâ”€â”€ crm_product_info_test.py
â”‚     â”œâ”€â”€ crm_sale_details_test.py
â”‚     â”œâ”€â”€ erp_customer_az12_test.py
â”‚     â”œâ”€â”€ erp_loc_a101_test.py
â”‚     â”œâ”€â”€ erp_px_cat_g1v2_test.py
â”‚     â””â”€â”€ print_sample_of_all.py
â”‚   â””â”€â”€ main.py                  # CLI Entry point
â””â”€â”€ requirements.txt         # Project dependencies
```
## ğŸ› ï¸ **<u>Data Pipeline Details</u>**
### **1. ğŸ¥‰ Bronze Layer (Raw Ingestion)**
- Source: Extracts data from multi-source CSV files (CRM & ERP).

- Process: Bulk loading into SQL Server using Polars for high-speed ingestion.

- Metadata: Every record is tagged with a bronze_inserted_at timestamp to enable incremental tracking.

### **2. ğŸ¥ˆ Silver Layer (Cleansing & Transformation)**
- Cleaning: Handles data types standardization and null values handling.

- Incremental Load (The Brain): The auto_increment.py engine compares the source (Bronze) with the target (Silver) using timestamps, ensuring that only new records are processed.

- Validation: Integrated Soda Core checks run at this stage to ensure data schema and integrity before moving forward.

### **3. ğŸ¥‡ Gold Layer (Analytical Modeling)**
- Storage: Data is moved to DuckDB for specialized analytical performance.

- Modeling: Implements a Star Schema (Fact & Dimension tables).

- Final Product: Creates SQL Views ready for BI tools like Power BI or Tableau.

### **ğŸ›¡ï¸ Reliability & Self-Healing**
What makes this framework different is the Self-Healing Infrastructure:

- Dependency Awareness: The system detects if you are trying to run a "Gold Load" while the "Silver" or "Bronze" layers are empty.

- Recursive Triggers: It will automatically backtrack, initialize the database, create schemas, and load the missing upstream data before completing your requested operation.

- Fail-Safe Connections: Robust handling of Windows Authentication and dynamic SQL Server connection strings.

### **âœ… Data Quality Assurance (Soda.io)**
- We don't just move data; we ensure it's correct. The pipeline executes automated tests:

- Schema Validation: Ensures no breaking changes in source files.

- Uniqueness Checks: Prevents duplicate records in Dimension tables.

- Referential Integrity: Validates that Sales facts correspond to existing Customers and Products.

### **ğŸš€ How to Run**
- Configure SQL Server: Ensure your SQL instance is running.

- Install Requirements: pip install -r requirements.txt

- Launch: Run python src/main.py.

**Interactive CLI: Follow the prompts to select the layer (Bronze/Silver/Gold) or perform a full "All Tables" sync.**

## âœ¨ Developed by
**Sohaib Omar**

Feel free to reach out for collaborations or questions!

**_LinkedIn_**:
ğŸ”—
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/sohaib-omar-188oo)
